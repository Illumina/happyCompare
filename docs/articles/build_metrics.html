<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Adding build metrics • happyCompare</title>
<!-- jquery --><script src="https://code.jquery.com/jquery-3.1.0.min.js" integrity="sha384-nrOSfDHtoPMzJHjVTdCopGqIqeYETSXhZDFyniQ8ZHcVy08QesyHcnOUpMpqnmWq" crossorigin="anonymous"></script><!-- Bootstrap --><link href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script><!-- Font Awesome icons --><link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" integrity="sha384-T8Gy5hrqNKT+hzMclPo118YTQO6cYprQmhrYwIiQ/3axmI1hQomh7Ud2hPOy8SP1" crossorigin="anonymous">
<!-- pkgdown --><link href="../pkgdown.css" rel="stylesheet">
<script src="../jquery.sticky-kit.min.js"></script><script src="../pkgdown.js"></script><!-- mathjax --><script src="https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <div class="container template-vignette">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="../index.html">happyCompare</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="..//index.html">
    <span class="fa fa-home fa-lg"></span>
     
  </a>
</li>
<li>
  <a href="../reference/index.html">Reference</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Articles
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../articles/build_metrics.html">Adding build metrics</a>
    </li>
    <li>
      <a href="../articles/simulations.html">Calibrating HDIs with simulated data</a>
    </li>
    <li>
      <a href="../articles/stratified_counts.html">Analysing stratified counts</a>
    </li>
  </ul>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right"></ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      
      </header><div class="row">
  <div class="col-md-9">
    <div class="page-header toc-ignore">
      <h1>Adding build metrics</h1>
                        <h4 class="author">Mar Gonzalez-Porta</h4>
            
            <h4 class="date">2017-09-07</h4>
          </div>

    
    
<div class="contents">
<p>Often we have additional information about our samples besides the results generated with hap.py, e.g. metadata, build metrics, etc. In this vignette we will illustrate how to incorporate custom metrics into our <code>happy_compare</code> object, just by adding an additional <code>build_metrics</code> column to our samplesheet (see <code>extdata/samplesheets/pcrfree_vs_nano.vignettes.csv</code> for an example).</p>
<div id="set-up" class="section level2">
<h2 class="hasAnchor">
<a href="#set-up" class="anchor"></a>Set up</h2>
<p>Let’s load our NovaSeq dataset with PCR-Free vs. Nano builds for NA12878, using a samplesheet that includes custom metrics:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># do not run</span>
samplesheet_path &lt;-<span class="st"> </span><span class="kw">system.file</span>(<span class="st">"extdata/samplesheets"</span>, <span class="st">"pcrfree_vs_nano.vignettes.csv"</span>,
                                <span class="dt">package =</span> <span class="st">"happyCompare"</span>)
happy_compare &lt;-<span class="st"> </span><span class="kw"><a href="../reference/read_samplesheet.html">read_samplesheet</a></span>(samplesheet_path)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># load from Rds object</span>
<span class="kw">download.file</span>(<span class="st">"https://raw.githubusercontent.com/Illumina/happyCompare/master/data-raw/stratified_counts/happy_compare.Rds"</span>, <span class="st">"happy_compare.Rds"</span>, <span class="dt">method =</span> <span class="st">"curl"</span>)
happy_compare &lt;-<span class="st"> </span><span class="kw">readRDS</span>(<span class="st">"happy_compare.Rds"</span>)</code></pre></div>
<p>Inspecting the elements of the resulting <code>happy_compare</code> object confirms that build metrics have been successfully imported:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">names</span>(happy_compare)</code></pre></div>
<pre><code>## [1] "samplesheet"   "happy_results" "build_metrics" "ids"</code></pre>
</div>
<div id="exploring-hap-py-results" class="section level2">
<h2 class="hasAnchor">
<a href="#exploring-hap-py-results" class="anchor"></a>Exploring hap.py results</h2>
<p>One of the first questions that we can ask given our demo dataset is whether library prep methods have any impact to variant calling performance. ROC curves are a convenient way of visualising accuracy across multiple classification methods (in our case True Positive vs. False Positive calls in PCR-Free vs. Nano builds), and we can quickly produce them from hap.py outputs with <code><a href="../reference/hc_plot_roc.html">hc_plot_roc()</a></code>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># extract metrics</span>
roc_snvs &lt;-<span class="st"> </span><span class="kw"><a href="../reference/extract_metrics.html">extract_metrics</a></span>(happy_compare, <span class="dt">table =</span> <span class="st">"pr.snp.pass"</span>)
roc_indels &lt;-<span class="st"> </span><span class="kw"><a href="../reference/extract_metrics.html">extract_metrics</a></span>(happy_compare, <span class="dt">table =</span> <span class="st">"pr.indel.pass"</span>)

<span class="co"># plot precision/recall curves</span>
p1 &lt;-<span class="st"> </span><span class="kw"><a href="../reference/hc_plot_roc.html">hc_plot_roc</a></span>(<span class="dt">happy_roc =</span> roc_snvs, <span class="dt">type =</span> <span class="st">"SNP"</span>, <span class="dt">filter =</span> <span class="st">"PASS"</span>, <span class="dt">ylim =</span> <span class="kw">c</span>(<span class="fl">0.995</span>, <span class="dv">1</span>))
p2 &lt;-<span class="st"> </span><span class="kw"><a href="../reference/hc_plot_roc.html">hc_plot_roc</a></span>(<span class="dt">happy_roc =</span> roc_indels, <span class="dt">type =</span> <span class="st">"INDEL"</span>, <span class="dt">filter =</span> <span class="st">"PASS"</span>)
gridExtra::<span class="kw">grid.arrange</span>(p1, p2, <span class="dt">ncol =</span> <span class="dv">2</span>)</code></pre></div>
<p><img src="build_metrics_files/figure-html/unnamed-chunk-5-1.png" width="672"></p>
<p>Note the difference in scales between the two plots, which highlights better performance overall for SNPs, regardless of analysis group. We also see that the differences between PCR-Free and Nano are most accentuated in INDELs, with a wider spread in precision across Nano builds.</p>
</div>
<div id="linking-hap-py-results-with-build-metrics" class="section level2">
<h2 class="hasAnchor">
<a href="#linking-hap-py-results-with-build-metrics" class="anchor"></a>Linking hap.py results with build metrics</h2>
<p>Since we have imported build metrics when creating our <code>happy_compare</code> object, we can follow up on the variability seen for INDEL precision in Nano. Let’s start by creating a combined dataset for hap.py and custom metrics:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># link build metrics to hap.py results</span>
summary &lt;-<span class="st"> </span><span class="kw"><a href="../reference/extract_metrics.html">extract_metrics</a></span>(happy_compare, <span class="dt">table =</span> <span class="st">"summary"</span>)
build_metrics &lt;-<span class="st"> </span><span class="kw"><a href="../reference/extract_metrics.html">extract_metrics</a></span>(happy_compare, <span class="dt">table =</span> <span class="st">"build.metrics"</span>)
merged_df &lt;-<span class="st"> </span>summary %&gt;%<span class="st"> </span>
<span class="st">  </span><span class="kw">filter</span>(Type ==<span class="st"> "INDEL"</span>, Filter ==<span class="st"> "PASS"</span>) %&gt;%<span class="st"> </span>
<span class="st">  </span><span class="kw">select</span>(happy_prefix, METRIC.Precision) %&gt;%<span class="st"> </span>
<span class="st">  </span><span class="kw">inner_join</span>(build_metrics)</code></pre></div>
<pre><code>## Joining, by = "happy_prefix"</code></pre>
<p>This gives us a large number of metrics to work with:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">dim</span>(merged_df)</code></pre></div>
<pre><code>## [1]  14 141</code></pre>
<p>From here, we can investigate the relationship between our build metrics and INDEL precision using a linear model. Since we don’t want to make any assumptions on variable importance, let’s use <code>regsubsets()</code> from the <code>leaps</code> package to identify the best predictors:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># clean up metrics dataset and focus on Nano builds for now</span>
subset_df &lt;-<span class="st"> </span>merged_df %&gt;%<span class="st"> </span>
<span class="st">  </span><span class="kw">filter</span>(Group.Id ==<span class="st"> "Nano"</span>) %&gt;%<span class="st"> </span>
<span class="st">  </span><span class="kw">select_if</span>(function(x) (<span class="kw">is.numeric</span>(x) &amp;&amp;<span class="st"> </span><span class="kw">sum</span>(!<span class="kw">is.na</span>(x)) &gt;<span class="st"> </span><span class="dv">0</span>) &amp;&amp;<span class="st"> </span><span class="kw">n_distinct</span>(x) &gt;<span class="st"> </span><span class="dv">1</span>)

<span class="co"># identify the best model for 1 to nvmax predictors</span>
nvmax &lt;-<span class="st"> </span><span class="dv">5</span>
fit &lt;-<span class="st"> </span>leaps::<span class="kw">regsubsets</span>(METRIC.Precision ~<span class="st"> </span>., <span class="dt">data =</span> subset_df, 
                         <span class="dt">method =</span> <span class="st">"backward"</span>, <span class="dt">nbest =</span> <span class="dv">1</span>, <span class="dt">nvmax =</span> nvmax)

<span class="co"># collect model statistics</span>
fit_stats &lt;-<span class="st"> </span><span class="kw">data.frame</span>(
  <span class="dt">nvars =</span> <span class="kw">factor</span>(<span class="dv">1</span>:nvmax),
  <span class="dt">bic =</span> <span class="kw">summary</span>(fit)$bic,
  <span class="dt">adjr2 =</span> <span class="kw">summary</span>(fit)$adjr2
) </code></pre></div>
<p>We now have <code>nvmax</code> models, ranging from 1 to <code>nvmax</code> predictor variables. We can assess how much we gain when introducing additional predictors by inspecting model statistics:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fit_stats %&gt;%<span class="st"> </span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> bic, <span class="dt">y =</span> adjr2)) +
<span class="st">  </span><span class="kw">geom_point</span>(<span class="kw">aes</span>(<span class="dt">color =</span> nvars), <span class="dt">size =</span> <span class="dv">3</span>) +
<span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">"Model accuracy varies with the number of predictor variables"</span>)</code></pre></div>
<p><img src="build_metrics_files/figure-html/unnamed-chunk-9-1.png" width="576"></p>
<p>From the plot we see that we do a good job at predicting INDEL precision with only one variable, and that adding more information into the model penalises both <span class="math inline">\(R^2\)</span> and <span class="math inline">\(BIC\)</span> score. We can learn about our best predictor by inspecting the first model:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">stats::<span class="kw">coef</span>(fit, <span class="dv">1</span>)</code></pre></div>
<pre><code>##          (Intercept) autosome_callability 
##          -5.76904999           0.06968588</code></pre>
<p>Overall, we have managed to identify callability as the main contributor to INDEL precision (in Nano builds). Finally, let’s confirm our predictions by visualising the relationship between these two variables in the full dataset:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">merged_df %&gt;%
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> autosome_callability, <span class="dt">y =</span> METRIC.Precision, <span class="dt">group =</span> Group.Id)) +
<span class="st">  </span><span class="kw">geom_point</span>(<span class="kw">aes</span>(<span class="dt">color =</span> Group.Id)) +
<span class="st">  </span><span class="kw">geom_smooth</span>(<span class="dt">method =</span> <span class="st">"lm"</span>, <span class="kw">aes</span>(<span class="dt">color =</span> Group.Id, <span class="dt">fill =</span> Group.Id), <span class="dt">lwd =</span> <span class="fl">0.25</span>) +
<span class="st">  </span><span class="kw">ylab</span>(<span class="st">"INDEL_precision"</span>) +
<span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">"Higher callability drives improvements in INDEL precision"</span>)</code></pre></div>
<p><img src="build_metrics_files/figure-html/unnamed-chunk-11-1.png" width="576"></p>
</div>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="sidebar">
        <div id="tocnav">
      <h2 class="hasAnchor">
<a href="#tocnav" class="anchor"></a>Contents</h2>
      <ul class="nav nav-pills nav-stacked">
<li><a href="#set-up">Set up</a></li>
      <li><a href="#exploring-hap-py-results">Exploring hap.py results</a></li>
      <li><a href="#linking-hap-py-results-with-build-metrics">Linking hap.py results with build metrics</a></li>
      </ul>
</div>
      </div>

</div>


      <footer><div class="copyright">
  <p>Developed by Mar Gonzalez-Porta, Ben Moore.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="http://hadley.github.io/pkgdown/">pkgdown</a>.</p>
</div>

      </footer>
</div>

  </body>
</html>
